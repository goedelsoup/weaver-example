/** DO NOT EDIT -- THIS FILE HAS BEEN GENERATED BY WEAVER */


/**
 * The response format that is requested.
 *
 * 
 *
 * * @example ["json"]
 *
 */
export const GEN_AI_OPENAI_REQUEST_RESPONSE_FORMAT = /** @type {const} */ 'gen_ai.openai.request.response_format';

/**
 * The service tier requested. May be a specific tier, default, or auto.
 *
 * 
 *
 * * @example ["auto", "default"]
 *
 */
export const GEN_AI_OPENAI_REQUEST_SERVICE_TIER = /** @type {const} */ 'gen_ai.openai.request.service_tier';

/**
 * The service tier used for the response.
 *
 * 
 *
 * * @example ["scale", "default"]
 *
 */
export const GEN_AI_OPENAI_RESPONSE_SERVICE_TIER = /** @type {const} */ 'gen_ai.openai.response.service_tier';

/**
 * A fingerprint to track any eventual change in the Generative AI environment.
 *
 * 
 *
 * * @example ["fp_44709d6fcb"]
 *
 */
export const GEN_AI_OPENAI_RESPONSE_SYSTEM_FINGERPRINT = /** @type {const} */ 'gen_ai.openai.response.system_fingerprint';

/**
 * The name of the operation being performed.
 *
 * 
 *
 * 
 */
export const GEN_AI_OPERATION_NAME = /** @type {const} */ 'gen_ai.operation.name';

/**
 * The encoding formats requested in an embeddings operation, if specified.
 *
 * 
 *
 * * @example [["base64"], ["float", "binary"]]
 *
 */
export const GEN_AI_REQUEST_ENCODING_FORMATS = /** @type {const} */ 'gen_ai.request.encoding_formats';

/**
 * The frequency penalty setting for the GenAI request.
 *
 * 
 *
 * * @example [0.1]
 *
 */
export const GEN_AI_REQUEST_FREQUENCY_PENALTY = /** @type {const} */ 'gen_ai.request.frequency_penalty';

/**
 * The maximum number of tokens the model generates for a request.
 *
 * 
 *
 * * @example [100]
 *
 */
export const GEN_AI_REQUEST_MAX_TOKENS = /** @type {const} */ 'gen_ai.request.max_tokens';

/**
 * The name of the GenAI model a request is being made to.
 *
 * 
 *
 * * @example gpt-4
 *
 */
export const GEN_AI_REQUEST_MODEL = /** @type {const} */ 'gen_ai.request.model';

/**
 * The presence penalty setting for the GenAI request.
 *
 * 
 *
 * * @example [0.1]
 *
 */
export const GEN_AI_REQUEST_PRESENCE_PENALTY = /** @type {const} */ 'gen_ai.request.presence_penalty';

/**
 * Requests with same seed value more likely to return same result.
 *
 * 
 *
 * * @example [100]
 *
 */
export const GEN_AI_REQUEST_SEED = /** @type {const} */ 'gen_ai.request.seed';

/**
 * List of sequences that the model will use to stop generating further tokens.
 *
 * 
 *
 * * @example [["forest", "lived"]]
 *
 */
export const GEN_AI_REQUEST_STOP_SEQUENCES = /** @type {const} */ 'gen_ai.request.stop_sequences';

/**
 * The temperature setting for the GenAI request.
 *
 * 
 *
 * * @example [0.0]
 *
 */
export const GEN_AI_REQUEST_TEMPERATURE = /** @type {const} */ 'gen_ai.request.temperature';

/**
 * The top_k sampling setting for the GenAI request.
 *
 * 
 *
 * * @example [1.0]
 *
 */
export const GEN_AI_REQUEST_TOP_K = /** @type {const} */ 'gen_ai.request.top_k';

/**
 * The top_p sampling setting for the GenAI request.
 *
 * 
 *
 * * @example [1.0]
 *
 */
export const GEN_AI_REQUEST_TOP_P = /** @type {const} */ 'gen_ai.request.top_p';

/**
 * Array of reasons the model stopped generating tokens, corresponding to each generation received.
 *
 * 
 *
 * * @example [["stop"], ["stop", "length"]]
 *
 */
export const GEN_AI_RESPONSE_FINISH_REASONS = /** @type {const} */ 'gen_ai.response.finish_reasons';

/**
 * The unique identifier for the completion.
 *
 * 
 *
 * * @example ["chatcmpl-123"]
 *
 */
export const GEN_AI_RESPONSE_ID = /** @type {const} */ 'gen_ai.response.id';

/**
 * The name of the model that generated the response.
 *
 * 
 *
 * * @example ["gpt-4-0613"]
 *
 */
export const GEN_AI_RESPONSE_MODEL = /** @type {const} */ 'gen_ai.response.model';

/**
 * The Generative AI product as identified by the client or server instrumentation.
 *
 * 
 *
 * * @example openai
 *
 */
export const GEN_AI_SYSTEM = /** @type {const} */ 'gen_ai.system';

/**
 * The type of token being counted.
 *
 * 
 *
 * * @example ["input", "output"]
 *
 */
export const GEN_AI_TOKEN_TYPE = /** @type {const} */ 'gen_ai.token.type';

/**
 * The number of tokens used in the GenAI input (prompt).
 *
 * 
 *
 * * @example [100]
 *
 */
export const GEN_AI_USAGE_INPUT_TOKENS = /** @type {const} */ 'gen_ai.usage.input_tokens';

/**
 * The number of tokens used in the GenAI response (completion).
 *
 * 
 *
 * * @example [180]
 *
 */
export const GEN_AI_USAGE_OUTPUT_TOKENS = /** @type {const} */ 'gen_ai.usage.output_tokens';
